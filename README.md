# ONE-PASS
---
This repository contains the source code for **ONE-PASS: Single Forward Pass Decoding for Listwise Reranking**, which is subbimted for peer review. The overall architecture is shown as follows:

<!-- <img src="./intro.png" width="45%"/> <img src="./main.png" width="45%"/> -->
| ![intro](./intro.png) | ![main](./main.png) |
|:--:|:--:|

 
---

### Requirement:
```python
transformers==4.50.0
torch==2.5.1
```

## Workflow
The following steps outline the process for data_prepare, training and evaluate ONE-PASS:

### Step 1: Data preprocessing
Get the auto-regressive ranking generated by the target LLM for fast training and evaluation:

- `./dataset/dataset_processing_Llama.ipynb`  (Already included in this repository, you can skip it)

### Step 2: Training and evaluation 
-  run `bash run.sh` with different settings.
-  See result in './log/$dataset/$backbone/$model_name', including log and model_save
 

## Ablation studies.
You can select dataset = ("rank_zephyr_IR_20" "quora_25" "ml-1m_25" "Games_25")  with variant as follows:
- --variant 'Permutation' 
- --variant 'Naive_add' 
- --variant 'wo_HAS'
- --variant ''wo_Mobius-1' 
- --variant ''wo_Mobius-2' 
- --variant ''ONE-PASS'
---
## baseline methods.
### Speculative methods:
```python
python baseline_Blockwise.py --data_name rank_zephyr_IR_20 --backbone Llama-3.2-3B-Instruct --n_step=2 # Blockwise
python baseline_Mesuda.py --data_name rank_zephyr_IR_20 --backbone Llama-3.2-3B-Instruct --n_step=2 # Medusa
python baseline_speculative_decoding.py --data_name rank_zephyr_IR_20 --backbone Llama-3.2-3B-Instruct --n_step=2 # SDM
python baseline_Parallel_Decoding.py --data_name rank_zephyr_IR_20 --backbone Llama-3.2-3B-Instruct --n_step=2  # PDM
```
### SFT methods Rankzephyr and First 
We implement the SFT methods Rankzephyr and First based on well-know tool torchtune
Required
```python
torchtune # release/0.5.0 !!!
```
First, set output_dir based on your preference (First or Rankzephyr), then for training.
#### Rankzephyr (training)
```python
 torchrun --nproc_per_node 4 full_finetune_distributed.py --config 3B_full.yaml # set  RankNet: False in 3B_full.yaml
```
#### First (training)
```python
 torchrun --nproc_per_node 4 full_finetune_distributed.py --config 3B_full.yaml # set  RankNet: True in 3B_full.yaml
```
#### Evaluation
```python 
 python evaluate_SFT model_path=./SFT_models/Llama-3.2-3B-Instruct/First
 python evaluate_SFT model_path=./SFT_models/Llama-3.2-3B-Instruct/Rankzephyr
```
