{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_from_string(string,mode='[]'):\n",
    "    letters = [char for char in string if char in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ']\n",
    "    return letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'rank_zephyr_IR'\n",
    "model_name = 'Llama-3.2-3B-Instruct' #must be it \n",
    "num_candidate  = 20\n",
    "new_tokens = ['%s'%chr(ord('A')+i) for i in range(num_candidate)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_content(text, num_candidate):\n",
    "    text = text.split('\\n')\n",
    "    content = []\n",
    "\n",
    "    # 构造替换字符串\n",
    "    pattern1 = r'(with\\s+)\\d+(\\s+passages)'\n",
    "    repl1 = lambda m: f\"{m.group(1)}{num_candidate}{m.group(2)}\"\n",
    "    content.append(re.sub(pattern1, repl1, text[0]))\n",
    "\n",
    "    content.append(text[1])\n",
    "\n",
    "    # 取中间 passage\n",
    "    content.extend(text[2:-2][:num_candidate])\n",
    "\n",
    "    content.append(text[-2])\n",
    "\n",
    "    pattern2 = r'(the\\s+)\\d+(\\s+passages)'\n",
    "    repl2 = lambda m: f\"{m.group(1)}{num_candidate}{m.group(2)}\"\n",
    "    content.append(re.sub(pattern2, repl2, text[-1]))\n",
    "\n",
    "    return '\\n'.join(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据读取\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from datasets import Dataset,load_dataset,load_from_disk\n",
    "\n",
    "\n",
    "# 获取上一级目录的路径\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, parent_dir)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # 只使用 GPU 3\n",
    "if data_name == 'rank_zephyr_IR':\n",
    "    ds = load_dataset(\"rryisthebest/rank_zephyr_training_data_alpha\")\n",
    "    train_data = ds['train']\n",
    "if data_name == 'quora':\n",
    "    train_data = load_from_disk(\"quora_original/quora\")\n",
    "if data_name == 'Games':\n",
    "    train_data = load_from_disk(\"Games_original/Games\")\n",
    "\n",
    "train_list = train_data.to_list()\n",
    "for index in range(len(train_list)):\n",
    "    prompt =  train_list[index]['conversations'][1]['value']\n",
    "    prompt = prompt.replace('The output format should be [] > [], e.g., [B] > [A]','')\n",
    "    prompt = prompt.replace('each indicated by a alphabetic identifier []','each indicated by an identifier infront of the sentence')\n",
    "    for letter in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\":\n",
    "        prompt = prompt.replace(\"[%s]\"%letter, f\"{letter}\") \n",
    "    prompt += \"\"\"\\nNotice: 1. Return a single line with the identifiers in descending order, separated by \"|\" (e.g.,STRICTLY OUTPUT FORMAT: |C|B|A|D|); 2. None space between the identifiers. 3. We have %s candidates in total, so the output should be a ranking of %s identifiers.\"\"\"%(num_candidate,num_candidate)\n",
    "        \n",
    "    train_list[index]['conversations'][1]['value'] = generate_content(prompt,num_candidate)\n",
    "train_data = Dataset.from_list(train_list)\n",
    "#去除特别少的<\n",
    "def func_1(example): # 去除有重复元素\n",
    "    extracted = extract_from_string(example['conversations'][2]['value'])\n",
    "    return len(extracted) >= 20#保留ranking大于10的部分   \n",
    "train_data = train_data.filter(func_1)\n",
    "\n",
    "\n",
    "train_list = train_data.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0]['conversations'][1]['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "# 加载 Tokenizer\n",
    "model_path = \"/data/zoo/%s\"%model_name\n",
    "def initialize_models(model_path):\n",
    "    \"\"\"Initialize base model and tokenizer\"\"\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    llm = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map='cuda',\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation='flash_attention_2',\n",
    "    )\n",
    "    # add new tokenizer\n",
    "    # old_tokens = [chr(ord('A')+i) for i in range(num_candidate)]\n",
    "    # tokenizer.add_tokens(new_tokens)\n",
    "    # llm.resize_token_embeddings(len(tokenizer))\n",
    "    # llm.eval()\n",
    "    # with torch.no_grad():\n",
    "    #     for i in range(num_candidate):\n",
    "    #         llm.lm_head.weight[tokenizer.encode('|%s|'%chr(ord('A')+i),add_special_tokens=False)] = llm.lm_head.weight[tokenizer.encode(chr(ord('A')+i),add_special_tokens=False)]\n",
    "    #         llm.model.embed_tokens.weight[tokenizer.encode('|%s|'%chr(ord('A')+i),add_special_tokens=False)] = llm.model.embed_tokens.weight[tokenizer.encode(chr(ord('A')+i),add_special_tokens=False)]\n",
    "    return llm, tokenizer\n",
    "llm,tokenizer = initialize_models(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 加载\n",
    "from utils import GenerationDataset, combined_collate_fn, sample_top_p\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "\n",
    "\n",
    "all_dataset = GenerationDataset(train_data, tokenizer, combined=True)\n",
    "all_dataloader = DataLoader(all_dataset, batch_size=1)\n",
    "#               \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Llama' in model_name:\n",
    "    end_token = '<|eot_id|>'\n",
    "if 'Qwen' in model_name:\n",
    "    end_token = '<|im_end|>'\n",
    "\n",
    "def create_logits_processor_llama(corpus,start):\n",
    "    corpus_id = set([tokenizer.convert_tokens_to_ids(letter) for letter in corpus]) #| \n",
    "    token_id4 = tokenizer.convert_tokens_to_ids(end_token)\n",
    "    token_id1 = tokenizer.convert_tokens_to_ids('|')\n",
    "    def process_token(token_ids, logits):\n",
    "        token_ids = token_ids[0][start:]\n",
    "        # print(token_ids)\n",
    "        if len(token_ids)%2 == 1:\n",
    "            logits[:,list(corpus_id)] += 50\n",
    "        else:\n",
    "            logits[:,token_id1] += 100\n",
    "        token_ids_set = set(token_ids.tolist())\n",
    "        exist_token = list(set(token_ids_set)&corpus_id)\n",
    "        # print(corpus_id)\n",
    "        # if not\n",
    "        logits[:,exist_token] = - 10000000\n",
    "        # print(token_ids)\n",
    "        # print(exist_token)\n",
    "        if len(exist_token) == len(corpus_id): #不把corpus_id中的token全部生成,就不能结束\n",
    "            logits[:,token_id4] = 1000\n",
    "        return logits\n",
    "    return process_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_self_regression_with_constraint(model,tokenizer,batch):\n",
    "    text,GT_label,ranking = batch\n",
    "    input_encoding = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                truncation=True,\n",
    "            )\n",
    "    # 获取input_ids\n",
    "    input_ids = input_encoding['input_ids'].cuda()\n",
    "    #指定生成的前几个token\n",
    "    # add_token = torch.tensor(tokenizer.encode('|')[1:])\n",
    "    #合并\n",
    "    # input_ids = torch.cat([input_ids, add_token.unsqueeze(0)],dim=1).cuda()     #指定生成的前几个token\n",
    "\n",
    "    input_length = input_ids.shape[1]\n",
    "\n",
    "    corpus = new_tokens\n",
    "    # print(corpus)\n",
    "    process_token = create_logits_processor_llama(corpus,input_length)\n",
    "\n",
    "    # 4. 生成文本\n",
    "    with torch.no_grad():  # 关闭梯度计算，减少显存占用\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids, \n",
    "            max_length=5000,                   # 与 sampling_params 的 max_tokens 对齐\n",
    "            do_sample=False,                   # temperature=0.0 时一般设置为False\n",
    "            temperature=0.0,                   # 完全确定性生成\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            logits_processor=[process_token]   # 与 sampling_params 中的 logits_processors 对齐\n",
    "        )\n",
    "\n",
    "\n",
    "    # 5. 解码输出\n",
    "    generated_ids = output_ids[:, input_length:]\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_self_regression(model,tokenizer,batch):\n",
    "    text,GT_label,ranking = batch\n",
    "    input_encoding = tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                truncation=True,\n",
    "            )\n",
    "    # 获取input_ids\n",
    "    input_ids = input_encoding['input_ids'].cuda()\n",
    "    #指定生成的前几个token\n",
    "    # add_token = torch.tensor(tokenizer.encode('|')[1:])\n",
    "    #合并\n",
    "    # input_ids = torch.cat([input_ids, add_token.unsqueeze(0)],dim=1).cuda()     #指定生成的前几个token\n",
    "\n",
    "    input_length = input_ids.shape[1]\n",
    "\n",
    "    corpus = new_tokens\n",
    "    # print(corpus)\n",
    "    process_token = create_logits_processor_llama(corpus,input_length)\n",
    "\n",
    "    # 4. 生成文本\n",
    "    with torch.no_grad():  # 关闭梯度计算，减少显存占用\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids, \n",
    "            max_length=5000,                   # 与 sampling_params 的 max_tokens 对齐\n",
    "            do_sample=False,                   # temperature=0.0 时一般设置为False\n",
    "            temperature=0.0,                   # 完全确定性生成\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            # logits_processor=[process_token]   # 与 sampling_params 中的 logits_processors 对齐\n",
    "        )\n",
    "\n",
    "\n",
    "    # 5. 解码输出\n",
    "    generated_ids = output_ids[:, input_length:]\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "break_flag = False\n",
    "wrong_cache = []\n",
    "index = -1\n",
    "for batch in tqdm(all_dataloader):\n",
    "    index += 1\n",
    "    GT_label = batch[1][0][: batch[1][0].rfind(']') + 1]\n",
    "    # generate_label1 = generate_text_self_regression(llm,tokenizer,batch)\n",
    "    generate_label = generate_text_self_regression_with_constraint(llm,tokenizer,batch)\n",
    "    generate_label_list = extract_from_string(generate_label,\"|\")\n",
    "    # print(GT_label)\n",
    "    # print(generate_label)\n",
    "    if set(generate_label_list) == set([chr(ord('A')+i) for i in range(num_candidate)]):\n",
    "        train_list[index]['conversations'][-1]['value'] = generate_label\n",
    "        # print(generate_label_list)\n",
    "    else:\n",
    "        print(\"Also unexpected error in revised generated label\")\n",
    "        # print(generate_label)       \n",
    "\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "save_data = Dataset.from_list(train_list)#.shuffle()\n",
    "save_data.save_to_disk(\"./%s_%s/%s\"%(data_name,num_candidate,model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "\n",
    "# data = load_from_disk(\"./rank_zephyr_IR_10/Llama-3.2-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-it\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-3-1b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "\n",
    "# def func_1(example): # 去除有重复元素\n",
    "#     extracted = extract_from_string(example['conversations'][2]['value'],'|')\n",
    "#     c1 = len(extracted) < 20\n",
    "#     c2 =  len(extracted) > 10\n",
    "#     return c1 and c2 #保留ranking大于10的部分   \n",
    "# ds = load_from_disk('/data/yingpeng/efficient_inference/dataset/rank_zephyr_IR_copy/Llama-3.2-3B-Instruct')\n",
    "# ds = ds.filter(func_1)\n",
    "\n",
    "# # Shuffle the dataset (set seed for reproducibility if desired)\n",
    "# ds_shuffled = ds.shuffle(seed=42)\n",
    "\n",
    "# # Save the shuffled dataset to disk\n",
    "# ds_shuffled.save_to_disk('/data/yingpeng/efficient_inference/dataset/rank_zephyr_IR_10/Llama-3.2-3B-Instruct')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">H>F>C>I>A>J>M>N>P>Q>R>S>T>U>V<W>X>Y>Z\n",
    ">H>F>C>I>A>J>M>N>P>S>T>R>B>D>L>K>E>O>G>Q\n",
    "--------------------------------\n",
    "  0%|          | 2/9978 [00:03<4:02:21,  1.46s/it]\n",
    ">F>A>I>J>L>O>P>S>T\n",
    ">F>A>I>J>L>O>P>S>T>R>K>H>C>B>M>D>E>G>N>Q\n",
    "--------------------------------\n",
    "  0%|          | 3/9978 [00:04<4:08:18,  1.49s/it]\n",
    ">C>A>E>H>J>O>N>P>S>T>Q>R>M>L>K>U>V>W>X>Y>Z\n",
    ">C>A>E>H>J>O>N>P>S>T>R>M>K>L>F>B>D>G>I>Q\n",
    "--------------------------------\n",
    "  0%|          | 4/9978 [00:05<3:55:05,  1.41s/it]\n",
    ">D>A>E>F>G>H>I>J>K>L>M>N>O>P>R>S>T\n",
    ">D>A>E>F>G>H>I>J>K>L>M>N>O>P>R>S>T>B>C>Q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchtune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
